#!/usr/bin/env python3
"""
Generate test data with embeddings for MT and LT memory layers.

This script creates structured test data with pre-computed embeddings
to enable semantic search testing.
"""

import json
from typing import List, Dict, Any
from datetime import datetime, timedelta
import random

# Try to use real embeddings
try:
    from utils.real_embedding import RealEmbeddingGenerator
    print("âœ… Using RealEmbeddingGenerator (sentence-transformers)")
    embedder = RealEmbeddingGenerator()
    use_real = True
except Exception as e:
    print(f"âš ï¸  RealEmbeddingGenerator not available: {e}")
    print("   Using mock embeddings")
    from utils import FakeEmbeddingGenerator
    embedder = FakeEmbeddingGenerator()
    use_real = False


# Sample conversations for different topics
CONVERSATIONS = {
    "ai_engineering": [
        "TÃ´i lÃ  AI Engineer, lÃ m viá»‡c vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n.",
        "CÃ´ng viá»‡c chÃ­nh cá»§a tÃ´i lÃ  training vÃ  fine-tuning LLM models.",
        "TÃ´i thÆ°á»ng sá»­ dá»¥ng PyTorch vÃ  Hugging Face Transformers.",
        "Vector embeddings vÃ  semantic search lÃ  ká»¹ nÄƒng quan trá»ng.",
        "RAG (Retrieval Augmented Generation) giÃºp cáº£i thiá»‡n chatbot.",
    ],
    "cooking": [
        "TÃ´i lÃ m Ä‘áº§u báº¿p táº¡i nhÃ  hÃ ng Sasukekeke á»Ÿ Nháº­t Báº£n.",
        "MÃ³n gÃ  chiÃªn (Karaage) lÃ  specialty cá»§a tÃ´i.",
        "CÃ´ng thá»©c gÃ  chiÃªn: Æ°á»›p gÃ  vá»›i gá»«ng, tá»i, nÆ°á»›c tÆ°Æ¡ng, sake.",
        "BÃ­ quyáº¿t lÃ  chiÃªn 2 láº§n: láº§n 1 nhiá»‡t Ä‘á»™ tháº¥p, láº§n 2 nhiá»‡t Ä‘á»™ cao.",
        "MÃ³n sushi vÃ  sashimi cÅ©ng ráº¥t Ä‘Æ°á»£c khÃ¡ch yÃªu thÃ­ch.",
        "NguyÃªn liá»‡u tÆ°Æ¡i sá»‘ng lÃ  yáº¿u tá»‘ quan trá»ng nháº¥t.",
    ],
    "gaming": [
        "TÃ´i chÆ¡i Valorant rank Immortal.",
        "Main agent cá»§a tÃ´i lÃ  Jett vÃ  Reyna.",
        "Aim training má»—i ngÃ y 30 phÃºt ráº¥t quan trá»ng.",
        "Map Ascent lÃ  map yÃªu thÃ­ch cá»§a tÃ´i.",
        "Team coordination vÃ  communication quyáº¿t Ä‘á»‹nh 70% tráº­n Ä‘áº¥u.",
    ],
    "technology": [
        "OpenAI GPT-4 lÃ  model máº¡nh nháº¥t hiá»‡n táº¡i.",
        "API rate limit lÃ  10,000 requests/minute cho tier 3.",
        "Token pricing: $0.03/1K input, $0.06/1K output tokens.",
        "Streaming responses cáº£i thiá»‡n user experience Ä‘Ã¡ng ká»ƒ.",
        "Fine-tuning costs $0.008/1K training tokens.",
    ],
    "travel": [
        "TÃ´i Ä‘Ã£ du lá»‹ch Tokyo vÃ o mÃ¹a xuÃ¢n nÄƒm ngoÃ¡i.",
        "Hoa anh Ä‘Ã o (sakura) á»Ÿ cÃ´ng viÃªn Ueno ráº¥t Ä‘áº¹p.",
        "Äi tÃ u Ä‘iá»‡n ngáº§m ráº¥t tiá»‡n lá»£i vÃ  Ä‘Ãºng giá».",
        "MÃ³n ramen Ichiran lÃ  must-try khi Ä‘áº¿n Tokyo.",
        "TeamLab Borderless museum cÃ³ triá»ƒn lÃ£m Ã¡nh sÃ¡ng tuyá»‡t vá»i.",
    ],
    "education": [
        "TÃ´i há»c chuyÃªn ngÃ nh Computer Science táº¡i Äáº¡i há»c BÃ¡ch Khoa.",
        "MÃ´n há»c yÃªu thÃ­ch lÃ  Machine Learning vÃ  NLP.",
        "Thesis cá»§a tÃ´i vá» Transformer architecture optimization.",
        "Äang lÃ m research vá» efficient attention mechanisms.",
        "Professor Nguyen lÃ  advisor ráº¥t tá»‘t.",
    ]
}


def generate_mid_term_chunks(num_chunks: int = 20) -> List[Dict[str, Any]]:
    """
    Generate mid-term memory chunks with embeddings.
    
    Each chunk represents a summary of 5-10 messages.
    """
    print(f"\nğŸ“¦ Generating {num_chunks} Mid-Term Memory chunks...")
    
    chunks = []
    topics = list(CONVERSATIONS.keys())
    base_time = datetime.now() - timedelta(days=7)
    
    for i in range(num_chunks):
        # Pick random topic
        topic = random.choice(topics)
        topic_messages = CONVERSATIONS[topic]
        
        # Sample messages for this chunk
        num_messages = random.randint(3, 5)
        chunk_messages = random.sample(topic_messages, min(num_messages, len(topic_messages)))
        
        # Create summary
        summary = f"[{topic.replace('_', ' ').title()}] {' | '.join(chunk_messages[:2])}"
        
        # Generate embedding
        print(f"   {i+1}/{num_chunks} Embedding: {summary[:50]}...")
        embedding = embedder.generate(summary)
        
        chunk = {
            "id": f"mtm_chunk_{i+1:03d}",
            "summary": summary,
            "embedding": embedding,
            "metadata": {
                "topic": topic,
                "message_count": num_messages,
                "timestamp": (base_time + timedelta(hours=i*2)).isoformat(),
                "importance": random.uniform(0.5, 1.0),
            },
            "original_messages": chunk_messages,
        }
        chunks.append(chunk)
    
    print(f"âœ… Generated {len(chunks)} MTM chunks")
    return chunks


def generate_long_term_facts(num_facts: int = 30) -> List[Dict[str, Any]]:
    """
    Generate long-term memory facts with embeddings.
    
    These are persistent knowledge items.
    """
    print(f"\nğŸ“š Generating {num_facts} Long-Term Memory facts...")
    
    facts = [
        # Personal info
        {"fact": "TÃªn tÃ´i lÃ  Innotech, AI Engineer vÃ  Ä‘áº§u báº¿p chuyÃªn nghiá»‡p", "category": "identity"},
        {"fact": "TÃ´i lÃ m viá»‡c vá»›i LLM models vÃ  RAG systems", "category": "work"},
        {"fact": "TÃ´i lÃ m Ä‘áº§u báº¿p táº¡i nhÃ  hÃ ng Sasukekeke á»Ÿ Nháº­t Báº£n", "category": "work"},
        {"fact": "Main agent Valorant cá»§a tÃ´i lÃ  Jett vÃ  Reyna, rank Immortal", "category": "gaming"},
        
        # Technical knowledge
        {"fact": "RAG = Retrieval Augmented Generation, káº¿t há»£p retrieval vÃ  generation", "category": "tech"},
        {"fact": "Vector embeddings convert text thÃ nh numerical vectors", "category": "tech"},
        {"fact": "Cosine similarity Ä‘o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a hai vectors", "category": "tech"},
        {"fact": "Sentence-transformers lÃ  library tá»‘t cho semantic search", "category": "tech"},
        {"fact": "GPT-4 context window lÃ  128K tokens", "category": "tech"},
        {"fact": "Fine-tuning requires minimum 10 high-quality examples", "category": "tech"},
        
        # Cooking knowledge
        {"fact": "GÃ  chiÃªn Nháº­t (Karaage) cáº§n Æ°á»›p vá»›i gá»«ng, tá»i, nÆ°á»›c tÆ°Æ¡ng", "category": "cooking"},
        {"fact": "ChiÃªn 2 láº§n: láº§n 1 nhiá»‡t Ä‘á»™ 160Â°C, láº§n 2 nhiá»‡t Ä‘á»™ 180Â°C", "category": "cooking"},
        {"fact": "Sushi rice cáº§n nÃªm vá»›i giáº¥m, Ä‘Æ°á»ng, muá»‘i theo tá»· lá»‡ 5:2:1", "category": "cooking"},
        {"fact": "CÃ¡ ngá»« (maguro) tá»‘t nháº¥t cho sashimi pháº£i Ä‘Æ°á»£c báº£o quáº£n -60Â°C", "category": "cooking"},
        {"fact": "Wasabi tháº­t Ä‘Æ°á»£c lÃ m tá»« rá»… cÃ¢y wasabi, ráº¥t Ä‘áº¯t tiá»n", "category": "cooking"},
        
        # Gaming knowledge
        {"fact": "Valorant cÃ³ 5 roles chÃ­nh: Duelist, Controller, Initiator, Sentinel", "category": "gaming"},
        {"fact": "Jett cÃ³ ability dash vÃ  updraft Ä‘á»ƒ di chuyá»ƒn nhanh", "category": "gaming"},
        {"fact": "Crosshair placement á»Ÿ head level giÃºp tÄƒng headshot rate", "category": "gaming"},
        {"fact": "Economy management: save round khi team cÃ³ < 2000 credits", "category": "gaming"},
        {"fact": "Spike plant takes 4 seconds, defuse takes 7 seconds (4 with kit)", "category": "gaming"},
        
        # Travel knowledge
        {"fact": "Tokyo cÃ³ 13 subway lines vÃ  JR Yamanote line loop", "category": "travel"},
        {"fact": "Sakura season á»Ÿ Tokyo thÆ°á»ng tá»« cuá»‘i thÃ¡ng 3 Ä‘áº¿n Ä‘áº§u thÃ¡ng 4", "category": "travel"},
        {"fact": "Shibuya crossing lÃ  nÆ¡i Ä‘Ã´ng Ä‘Ãºc nháº¥t Tokyo vá»›i 2500 ngÆ°á»i/láº§n", "category": "travel"},
        {"fact": "Tsukiji outer market má»Ÿ cá»­a 5am, tá»‘t nháº¥t Ä‘áº¿n sá»›m", "category": "travel"},
        {"fact": "Suica card hoáº¡t Ä‘á»™ng cho táº¥t cáº£ tÃ u Ä‘iá»‡n vÃ  nhiá»u cá»­a hÃ ng", "category": "travel"},
        
        # Education
        {"fact": "Transformer architecture gá»“m encoder vÃ  decoder vá»›i attention mechanism", "category": "education"},
        {"fact": "Self-attention complexity lÃ  O(nÂ²) vá»›i n lÃ  sequence length", "category": "education"},
        {"fact": "BERT uses masked language modeling for pre-training", "category": "education"},
        {"fact": "Learning rate scheduling giÃºp training á»•n Ä‘á»‹nh hÆ¡n", "category": "education"},
        {"fact": "Gradient clipping prevents exploding gradients trong RNN", "category": "education"},
    ]
    
    ltm_facts = []
    for i, fact_data in enumerate(facts[:num_facts]):
        fact = fact_data["fact"]
        category = fact_data["category"]
        
        print(f"   {i+1}/{num_facts} Embedding: {fact[:50]}...")
        embedding = embedder.generate(fact)
        
        ltm_fact = {
            "id": f"ltm_fact_{i+1:03d}",
            "content": fact,
            "embedding": embedding,
            "metadata": {
                "category": category,
                "importance": random.uniform(0.7, 1.0),
                "access_count": random.randint(1, 50),
                "last_accessed": (datetime.now() - timedelta(days=random.randint(0, 30))).isoformat(),
                "created_at": (datetime.now() - timedelta(days=random.randint(30, 180))).isoformat(),
            }
        }
        ltm_facts.append(ltm_fact)
    
    print(f"âœ… Generated {len(ltm_facts)} LTM facts")
    return ltm_facts


def calculate_statistics(data: List[Dict[str, Any]], data_type: str) -> Dict[str, Any]:
    """Calculate statistics about the generated data."""
    if not data:
        return {}
    
    embedding_field = 'embedding'
    content_field = 'summary' if data_type == 'MTM' else 'content'
    
    # Get embedding dimension
    if data and embedding_field in data[0]:
        embedding_dim = len(data[0][embedding_field])
    else:
        embedding_dim = 0
    
    # Average content length
    avg_length = sum(len(item.get(content_field, '')) for item in data) / len(data)
    
    return {
        "total_items": len(data),
        "embedding_dimension": embedding_dim,
        "avg_content_length": int(avg_length),
        "has_embeddings": all(embedding_field in item for item in data),
        "categories": list(set(item.get('metadata', {}).get('category', 'unknown') for item in data if 'metadata' in item)),
    }


def main():
    """Generate all test data with embeddings."""
    
    print("=" * 80)
    print("ğŸš€ GENERATING EMBEDDED TEST DATA FOR MEMORY LAYER LAB")
    print("=" * 80)
    
    if use_real:
        print(f"ğŸ“¦ Model: sentence-transformers")
        print(f"ğŸ“ Embedding dimension: {embedder.embedding_dim}")
    else:
        print(f"âš ï¸  Using mock embeddings (install sentence-transformers for real)")
    
    # Generate data
    mtm_chunks = generate_mid_term_chunks(num_chunks=20)
    ltm_facts = generate_long_term_facts(num_facts=30)
    
    # Calculate statistics
    mtm_stats = calculate_statistics(mtm_chunks, 'MTM')
    ltm_stats = calculate_statistics(ltm_facts, 'LTM')
    
    # Create metadata
    metadata = {
        "generated_at": datetime.now().isoformat(),
        "generator_version": "1.0",
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2" if use_real else "mock",
        "embedding_dimension": embedder.embedding_dim,
        "statistics": {
            "mid_term": mtm_stats,
            "long_term": ltm_stats,
        }
    }
    
    # Save data
    print("\nğŸ’¾ Saving data...")
    
    # Save MTM chunks
    mtm_file = "data/mid_term_chunks.json"
    with open(mtm_file, 'w', encoding='utf-8') as f:
        json.dump({
            "metadata": metadata,
            "chunks": mtm_chunks
        }, f, ensure_ascii=False, indent=2)
    print(f"   âœ… Saved {len(mtm_chunks)} MTM chunks to {mtm_file}")
    
    # Save LTM facts
    ltm_file = "data/long_term_facts.json"
    with open(ltm_file, 'w', encoding='utf-8') as f:
        json.dump({
            "metadata": metadata,
            "facts": ltm_facts
        }, f, ensure_ascii=False, indent=2)
    print(f"   âœ… Saved {len(ltm_facts)} LTM facts to {ltm_file}")
    
    # Print summary
    print("\n" + "=" * 80)
    print("ğŸ“Š GENERATION SUMMARY")
    print("=" * 80)
    print(f"\nâœ… Mid-Term Memory (MTM):")
    print(f"   Total chunks: {mtm_stats['total_items']}")
    print(f"   Embedding dim: {mtm_stats['embedding_dimension']}")
    print(f"   Avg length: {mtm_stats['avg_content_length']} chars")
    print(f"   Has embeddings: {mtm_stats['has_embeddings']}")
    
    print(f"\nâœ… Long-Term Memory (LTM):")
    print(f"   Total facts: {ltm_stats['total_items']}")
    print(f"   Embedding dim: {ltm_stats['embedding_dimension']}")
    print(f"   Avg length: {ltm_stats['avg_content_length']} chars")
    print(f"   Has embeddings: {ltm_stats['has_embeddings']}")
    print(f"   Categories: {', '.join(ltm_stats['categories'])}")
    
    print("\nğŸ‰ Data generation complete!")
    print("\nğŸ’¡ Next steps:")
    print("   1. Run tests: python3 test_comprehensive.py")
    print("   2. Test semantic search with real queries")
    print("   3. Compare relevance scores before/after")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()
