# Memory Layer Lab - Complete System Configuration
# Adjust these parameters for experimentation

# ============================================================================
# MEMORY LAYER SETTINGS
# ============================================================================

short_term_memory:
  max_items: 10                    # Max messages in STM
  retention_time: 300              # Seconds to keep in STM
  auto_compress: true              # Auto compress to MTM when full
  
mid_term_memory:
  max_chunks: 100                  # Max chunks in MTM
  chunk_size: 5                    # Messages per chunk
  retention_time: 3600             # Seconds (1 hour)
  auto_archive: true               # Auto move to LTM when full
  importance_threshold: 0.5        # Min importance to keep
  
long_term_memory:
  enabled: true                    # Enable LTM
  use_neo4j: false                # Use Neo4j (requires setup)
  max_facts: 1000                  # Max facts in memory-based LTM
  importance_threshold: 0.7        # Min importance to store
  auto_deduplicate: true          # Remove duplicates

# ============================================================================
# COMPRESSION SETTINGS
# ============================================================================

compression:
  enabled: true
  strategy: "score_based"          # score_based, mmr, truncate
  max_tokens: 1000                 # Max tokens after compression
  preserve_recent: 3               # Always preserve N most recent items
  preserve_important: true         # Always preserve high importance items
  importance_weight: 0.4           # Weight for importance (0.0-1.0)
  recency_weight: 0.3             # Weight for recency (0.0-1.0)
  relevance_weight: 0.3           # Weight for relevance (0.0-1.0)
  
  # MMR (Maximal Marginal Relevance) settings
  mmr:
    lambda_param: 0.5              # Balance relevance vs diversity (0=diverse, 1=relevant)
    
  # Truncate settings
  truncate:
    keep_first_n: 2                # Keep first N items
    keep_last_n: 3                 # Keep last N items

# ============================================================================
# SEMANTIC SEARCH SETTINGS
# ============================================================================

semantic_search:
  enabled: true
  use_real_embeddings: true        # Use sentence-transformers (requires install)
  embedding_model: "all-MiniLM-L6-v2"  # Model name
  embedding_dim: 384               # Embedding dimension
  similarity_threshold: 0.6        # Min similarity to consider relevant
  top_k_stm: 5                    # Top K from STM
  top_k_mtm: 3                    # Top K from MTM
  top_k_ltm: 5                    # Top K from LTM
  
  # Hybrid search (semantic + keyword)
  hybrid:
    enabled: false                 # Enable hybrid search
    semantic_weight: 0.7           # Weight for semantic (0.0-1.0)
    keyword_weight: 0.3            # Weight for keyword (0.0-1.0)

# ============================================================================
# CONTEXT AGGREGATION SETTINGS
# ============================================================================

aggregation:
  # Layer weights (must sum to 1.0)
  weights:
    stm: 0.5                       # Short-term weight
    mtm: 0.3                       # Mid-term weight
    ltm: 0.2                       # Long-term weight
    
  # Deduplication
  deduplication:
    enabled: true
    similarity_threshold: 0.9      # Consider duplicate if similarity > threshold
    strategy: "keep_highest_score" # keep_highest_score, keep_most_recent, merge
    
  # Context limits
  max_total_items: 50              # Max items after aggregation
  max_total_tokens: 2000           # Max tokens after aggregation

# ============================================================================
# RESPONSE GENERATION SETTINGS
# ============================================================================

response_generation:
  model: "gpt-4o-mini"             # LLM model
  temperature: 0.7                 # Creativity (0.0-2.0)
  max_tokens: 500                  # Max response length
  top_p: 0.9                       # Nucleus sampling
  frequency_penalty: 0.0           # Penalize repetition (-2.0 to 2.0)
  presence_penalty: 0.0            # Encourage new topics (-2.0 to 2.0)
  
  # System prompt
  system_prompt: |
    You are a helpful AI assistant with access to conversation history.
    Use the provided context to give informed, relevant responses.
    Be concise but thorough. Acknowledge when you use past information.

# ============================================================================
# INPUT PREPROCESSING SETTINGS
# ============================================================================

preprocessing:
  intent_detection: true           # Detect user intent
  keyword_extraction: true         # Extract keywords
  entity_recognition: false        # Extract entities (requires NER model)
  sentiment_analysis: false        # Analyze sentiment (requires model)
  
  # Intent categories
  intent_categories:
    - question
    - command
    - statement
    - greeting
    - farewell
    - other

# ============================================================================
# LANGFUSE TRACING SETTINGS
# ============================================================================

langfuse:
  enabled: false                   # Enable Langfuse tracing
  public_key: ""                   # Your Langfuse public key
  secret_key: ""                   # Your Langfuse secret key
  host: "https://cloud.langfuse.com"
  debug: false
  sample_rate: 1.0                 # Trace 100% of requests (0.0-1.0)
  environment: "development"       # development, staging, production
  
  # What to trace
  trace_llm_calls: true
  trace_embeddings: true
  trace_retrievals: true
  trace_full_pipeline: true
  
  # Privacy
  mask_user_content: false         # Mask user messages in traces
  mask_api_keys: true

# ============================================================================
# NEO4J SETTINGS (for LTM)
# ============================================================================

neo4j:
  enabled: false                   # Enable Neo4j
  uri: "bolt://localhost:7687"
  username: "neo4j"
  password: ""
  database: "neo4j"
  
  # Connection pool
  max_connection_lifetime: 3600
  max_connection_pool_size: 50
  connection_acquisition_timeout: 60
  encrypted: false

# ============================================================================
# PERFORMANCE SETTINGS
# ============================================================================

performance:
  # Caching
  cache:
    enabled: true
    max_size: 100                  # Max cached items
    ttl: 300                       # Time to live (seconds)
    
  # Batch processing
  batch:
    enabled: true
    batch_size: 10                 # Items per batch
    
  # Async operations
  async:
    enabled: false                 # Enable async operations
    max_workers: 4                 # Max concurrent workers

# ============================================================================
# LOGGING SETTINGS
# ============================================================================

logging:
  level: "INFO"                    # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "detailed"               # simple, detailed, json
  file: "logs/memory_layer.log"
  max_size: "10MB"
  backup_count: 5
  console: true                    # Also log to console

# ============================================================================
# EXPERIMENT SETTINGS (for A/B testing)
# ============================================================================

experiments:
  enabled: false
  active_experiments: []
  
  # Example experiment:
  # - name: "compression_strategy_test"
  #   variants:
  #     A: {strategy: "score_based"}
  #     B: {strategy: "mmr"}
  #   traffic_split: {A: 0.5, B: 0.5}

# ============================================================================
# UI SETTINGS (for Gradio interface)
# ============================================================================

ui:
  theme: "soft"                    # Gradio theme
  title: "Memory Layer Lab"
  description: "Experimental chatbot with multi-layer memory"
  show_config_panel: true          # Show config adjustment panel
  show_memory_viewer: true         # Show memory contents
  show_metrics: true               # Show performance metrics
  
  # Advanced features
  features:
    export_conversation: true
    import_conversation: true
    reset_memory: true
    download_logs: true

# ============================================================================
# SAFETY & LIMITS
# ============================================================================

safety:
  max_message_length: 2000         # Max chars per message
  max_conversation_length: 100     # Max messages in conversation
  rate_limit:
    enabled: false
    max_requests: 60               # Per time window
    time_window: 60                # Seconds
    
  content_filtering:
    enabled: false                 # Enable content filtering
    block_patterns: []             # Regex patterns to block
