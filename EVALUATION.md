# üìä Evaluation Tools Guide

Complete guide for evaluating and tuning Memory Layer parameters.

---

## üéØ Overview

The evaluation suite helps you:
- **Measure** retrieval quality & compression efficiency
- **Tune** parameters for optimal performance
- **Compare** different configurations
- **Prepare** for production deployment

---

## üõ†Ô∏è Tools

### 1. `evaluate_memory.py` - Full Evaluation

**Purpose:** Comprehensive assessment of memory layer performance

**Usage:**
```bash
python3 evaluate_memory.py
```

**What it measures:**
- ‚úÖ Retrieval quality (Precision, Recall, F1)
- ‚úÖ Source coverage (STM, MTM, LTM usage)
- ‚úÖ Context compression efficiency
- ‚úÖ Query latency
- ‚úÖ Token usage

**Output:**
```
üìä SUMMARY
Avg F1 Score:      0.850
Avg Precision:     0.867
Avg Recall:        0.833
Source Coverage:   0.900
Avg Latency:       45.23ms
Tests Passed:      5/5
Compression:       62.3% savings
Grade:             üü¢ EXCELLENT
```

**Report:** `evaluation_report_YYYYMMDD_HHMMSS.json`

---

### 2. `tune_parameters.py` - Parameter Optimization

**Purpose:** Find optimal parameter values automatically

**Usage:**
```bash
# Grid search (recommended)
python3 tune_parameters.py grid

# Tune specific parameter
python3 tune_parameters.py stm    # Tune STM size
python3 tune_parameters.py mtm    # Tune MTM size
python3 tune_parameters.py topk   # Tune top_k
```

**Grid Search:**
Tests combinations of:
- STM size: 5, 10, 15
- MTM size: 10, 20, 30
- top_k: 2, 3, 5

Total: 27 configurations

**Output:**
```
üìä TUNING RESULTS
Best Config:
  - STM size: 10
  - MTM size: 20
  - top_k: 3
  - F1 Score: 0.875

Top 5 configurations:
1. STM=10, MTM=20, top_k=3 ‚Üí F1=0.875
2. STM=15, MTM=20, top_k=3 ‚Üí F1=0.867
3. STM=10, MTM=30, top_k=3 ‚Üí F1=0.858
...
```

**Report:** `tuning_report_YYYYMMDD_HHMMSS.json`

---

### 3. `compare_configs.py` - Config Comparison

**Purpose:** A/B test multiple configurations side-by-side

**Usage:**
```bash
# Compare preset configs
python3 compare_configs.py preset

# Compare custom configs (edit script)
python3 compare_configs.py
```

**Preset Configs:**
- **Minimal**: Low memory (STM=5, MTM=10, top_k=2)
- **Balanced**: Default (STM=10, MTM=20, top_k=3)
- **Quality**: High recall (STM=15, MTM=30, top_k=5)
- **Performance**: Low latency (STM=5, MTM=15, top_k=2)

**Output:**
```
üìä COMPARISON TABLE
Metric                    Config 1     Config 2     Config 3
--------------------------------------------------------------
F1 Score                  0.750        0.850        0.883
Precision                 0.783        0.867        0.900
Recall                    0.717        0.833        0.867
Source Coverage           0.867        0.900        0.933
Pass Rate                 80.00%       100.00%      100.00%
Avg Latency (ms)          32.50        45.23        67.89
Compression Ratio         0.420        0.377        0.310
Token Savings %           58.0         62.3         69.0

üí° RECOMMENDATION
üèÜ Best F1 Score: Config 3
‚ö° Fastest: Config 1
üíæ Best Compression: Config 3
üéØ Best Overall: Config 2
```

**Report:** `comparison_report_YYYYMMDD_HHMMSS.json`

---

## üìã Evaluation Metrics Explained

### Retrieval Quality

**Precision:**
```
Precision = Found Relevant / Total Retrieved
```
- How many retrieved items are actually relevant?
- Higher = Less noise
- Target: ‚â• 0.8

**Recall:**
```
Recall = Found Relevant / Total Relevant
```
- How many relevant items did we find?
- Higher = Better coverage
- Target: ‚â• 0.7

**F1 Score:**
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```
- Balanced measure
- Main metric for optimization
- Target: ‚â• 0.75

**Source Coverage:**
```
Coverage = Layers Used / Expected Layers
```
- Are all memory layers utilized?
- Shows multi-layer integration
- Target: ‚â• 0.8

### Context Compression

**Compression Ratio:**
```
Ratio = Compressed Tokens / Original Tokens
```
- Lower = More compression
- Typical: 0.3 - 0.5 (50-70% savings)

**Token Savings:**
```
Savings % = (1 - Compression Ratio) * 100
```
- How much context reduced?
- Target: ‚â• 40%

### Performance

**Latency:**
- Time to retrieve memories
- Target: ‚â§ 100ms (production)
- Optimal: ‚â§ 50ms

**Pass Rate:**
```
Pass Rate = Tests Passed / Total Tests
```
- Tests meeting minimum relevance
- Target: ‚â• 80%

---

## üéØ Tuning Workflow

### Step 1: Baseline Evaluation
```bash
python3 evaluate_memory.py
```

Review metrics:
- Is F1 score acceptable (‚â• 0.75)?
- Is latency acceptable (‚â§ 100ms)?
- Is compression good (‚â• 40%)?

### Step 2: Identify Issues

**Low F1 Score (<0.75):**
- Increase `top_k` ‚Üí More results
- Increase memory sizes ‚Üí More context

**High Latency (>100ms):**
- Decrease memory sizes ‚Üí Less to search
- Decrease `top_k` ‚Üí Fewer comparisons

**Poor Compression (<40%):**
- Decrease STM size ‚Üí Less raw data
- Tune MTM summarization

### Step 3: Grid Search
```bash
python3 tune_parameters.py grid
```

Let it find optimal combination automatically.

### Step 4: Compare Top Configs
```bash
python3 compare_configs.py
```

Edit script to compare top 3 configs from grid search.

### Step 5: Validate
```bash
python3 test_code_analysis.py
```

Test with real queries to ensure quality.

---

## üìä Interpreting Results

### Excellent Performance
```
‚úÖ F1 ‚â• 0.85
‚úÖ Latency ‚â§ 50ms
‚úÖ Compression ‚â• 60%
‚úÖ Pass Rate 100%
‚Üí Grade: üü¢ EXCELLENT
```

### Good Performance
```
‚úÖ F1 ‚â• 0.70
‚úÖ Latency ‚â§ 80ms
‚úÖ Compression ‚â• 50%
‚úÖ Pass Rate ‚â• 80%
‚Üí Grade: üü° GOOD
```

### Needs Improvement
```
‚ö†Ô∏è  F1 < 0.60
‚ö†Ô∏è  Latency > 100ms
‚ö†Ô∏è  Compression < 40%
‚ö†Ô∏è  Pass Rate < 70%
‚Üí Grade: üî¥ NEEDS IMPROVEMENT
```

---

## üîß Common Tuning Scenarios

### Scenario 1: Low Memory Environment

**Goal:** Minimize memory footprint

**Config:**
```python
{
    'stm_max_items': 5,
    'mtm_max_chunks': 10,
    'top_k': 2
}
```

**Expected:**
- F1: ~0.70
- Latency: <40ms
- Memory: Low

### Scenario 2: High Quality Requirements

**Goal:** Maximum recall, F1 > 0.85

**Config:**
```python
{
    'stm_max_items': 15,
    'mtm_max_chunks': 30,
    'top_k': 5
}
```

**Expected:**
- F1: ~0.85+
- Latency: ~70ms
- Memory: High

### Scenario 3: Balanced (Default)

**Goal:** Good balance of all metrics

**Config:**
```python
{
    'stm_max_items': 10,
    'mtm_max_chunks': 20,
    'top_k': 3
}
```

**Expected:**
- F1: ~0.80
- Latency: ~50ms
- Memory: Medium

### Scenario 4: Low Latency

**Goal:** Minimize response time

**Config:**
```python
{
    'stm_max_items': 5,
    'mtm_max_chunks': 15,
    'top_k': 2
}
```

**Expected:**
- F1: ~0.72
- Latency: <35ms
- Memory: Low

---

## üìà Tracking Improvements

### Baseline ‚Üí Tuned Example

**Before (Default):**
```
F1 Score:      0.722
Latency:       52.34ms
Compression:   57.8%
Pass Rate:     60%
```

**After (Tuned):**
```
F1 Score:      0.850  (+17.7%)
Latency:       45.23ms (-13.6%)
Compression:   62.3%  (+7.8%)
Pass Rate:     100%   (+66.7%)
```

**Config Change:**
- STM: 10 ‚Üí 10 (no change)
- MTM: 20 ‚Üí 20 (no change)
- top_k: 2 ‚Üí 3 (+1)

**Lesson:** Small parameter changes can have big impact!

---

## üöÄ Quick Reference

```bash
# Full evaluation
python3 evaluate_memory.py

# Find best config
python3 tune_parameters.py grid

# Compare configs
python3 compare_configs.py preset

# Test with queries
python3 test_code_analysis.py

# Check integration
python3 check_integration.py

# Generate fresh data
python3 generate_data.py

# Validate schemas
python3 utils/schema_validator.py
```

---

## üìù Reports Generated

All tools generate timestamped JSON reports:

```
evaluation_report_20251001_142530.json
tuning_report_20251001_143015.json
comparison_report_20251001_143245.json
```

**Use these for:**
- Tracking improvements over time
- Comparing different data sets
- Documentation & audits
- Production deployment decisions

---

## üéØ Production Readiness

### Minimum Requirements
- ‚úÖ F1 Score: ‚â• 0.70
- ‚úÖ Latency: ‚â§ 100ms
- ‚úÖ Compression: ‚â• 40%
- ‚úÖ Pass Rate: ‚â• 80%

### Checklist
- [ ] Run full evaluation
- [ ] Tune parameters
- [ ] Compare top configs
- [ ] Validate with real queries
- [ ] Document chosen config
- [ ] Save evaluation reports

### Deploy When:
- All metrics meet minimum requirements
- Config validated with real-world queries
- Reports archived for reference

---

**Status:** ‚úÖ Evaluation Suite Complete  
**Next:** Run evaluations and tune for your domain  
**Goal:** Production-ready configuration for Innocody
