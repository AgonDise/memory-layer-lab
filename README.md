# ðŸ§  Memory Layer Lab

**Experimental chatbot with multi-layer memory system**

A modular, configurable chatbot with:
- ðŸ”„ Multi-layer memory (STM, MTM, LTM)
- ðŸ” Semantic search with embeddings
- ðŸ“Š Langfuse tracing & observability
- ðŸ—„ï¸ Neo4j support (optional)
- ðŸŽ›ï¸ Easy configuration via UI

**âœ… STATUS: Ready for experimentation!**

## Project Structure

```
memory_layer_lab/
â”‚
â”œâ”€â”€ main.py                 # Entrypoint for the chatbot
â”œâ”€â”€ config.py               # Configuration (memory size, DB, API keys)
â”œâ”€â”€ demo_workflow.py        # Demo of complete workflow
â”œâ”€â”€ test_simple.py          # Simple tests
â”‚
â”œâ”€â”€ core/                   # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ short_term.py       # Short-term memory (with embedding search)
â”‚   â”œâ”€â”€ mid_term.py         # Mid-term memory (enhanced with Neo4j MTM)
â”‚   â”œâ”€â”€ long_term.py        # Long-term memory (enhanced with Neo4j + VectorDB)
â”‚   â”œâ”€â”€ summarizer.py       # Summarization module
â”‚   â”œâ”€â”€ orchestrator.py     # Memory layer orchestration
â”‚   â”œâ”€â”€ preprocessor.py     # Input preprocessing & embeddings
â”‚   â”œâ”€â”€ aggregator.py       # Multi-layer context aggregation
â”‚   â”œâ”€â”€ compressor.py       # Context compression
â”‚   â””â”€â”€ synthesizer.py      # Response synthesis & formatting
â”‚
â”œâ”€â”€ mtm/                    # NEW: Mid-term Memory with Neo4j
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ temporal_graph.py   # Temporal graph (commit timeline)
â”‚   â”œâ”€â”€ knowledge_graph.py  # Knowledge graph (code relationships)
â”‚   â””â”€â”€ query.py            # Unified MTM query interface
â”‚
â”œâ”€â”€ ltm/                    # NEW: Long-term Memory with Neo4j + VectorDB
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ knowledge_graph.py  # LTM knowledge graph (design docs, concepts)
â”‚   â”œâ”€â”€ vecdb.py            # Vector database (FAISS/ChromaDB/Qdrant)
â”‚   â””â”€â”€ query.py            # Unified LTM query interface
â”‚
â”œâ”€â”€ bot/                   
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chatbot.py         # ChatBot class (advanced workflow)
â”‚   â””â”€â”€ response.py        # Response generation
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ logger.py          # Logging utilities
    â””â”€â”€ storage.py         # Storage utilities (file/DB)
```

## ðŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. (Optional) Install semantic search
pip install sentence-transformers scikit-learn

# 3. (Optional) Install Langfuse for tracing
pip install langfuse

# 4. Run chatbot
python main.py

# 5. (Optional) Launch config UI
python config_ui.py  # http://localhost:7861
```

## âš™ï¸ Configuration

### Via Config UI (Recommended)
```bash
python config_ui.py  # http://localhost:7861
```
Adjust parameters via web interface:
- Compression settings
- Memory layer sizes
- Semantic search thresholds
- LLM parameters

### Via Config File
Edit `config/system_config.yaml`:
```yaml
compression:
  strategy: "score_based"  # or mmr, truncate
  max_tokens: 1000
  
semantic_search:
  enabled: true
  similarity_threshold: 0.6
  
response_generation:
  model: "gpt-4o-mini"
  temperature: 0.7
```

### Langfuse Tracing (Optional)
1. Get API keys from https://cloud.langfuse.com
2. Edit `config/langfuse_config.yaml`
3. Set `langfuse.enabled: true` in system_config.yaml

### Neo4j (Optional)
1. Install Neo4j
2. Edit `config/neo4j_config.yaml`
3. Run `python test_neo4j_connection.py`

## âœ¨ Features

### ðŸ§  Memory Layers
- **STM**: Recent messages (default: 10)
- **MTM**: Summarized chunks (default: 100)
- **LTM**: Long-term facts with Neo4j support

### ðŸ” Semantic Search
- Real embeddings with sentence-transformers
- Cosine similarity ranking
- Top-K retrieval from each layer

### ðŸ—œï¸ Context Compression
- Multiple strategies (score-based, MMR, truncate)
- Configurable weights (importance, recency, relevance)
- Token budget management
### ðŸ“Š Observability
- Langfuse integration for tracing
- Performance metrics
- LLM call tracking

## ðŸ’» Usage Examples

### Run Tests
```bash
# Generate embedded test data
python generate_embedded_data.py

# Test semantic search
python test_semantic_search.py

# Test Neo4j connection
python test_neo4j_connection.py

# Run comprehensive tests
python test_comprehensive.py
```

### Adjust Config Programmatically
```python
from utils.config_manager import get_config

config = get_config()

# Get value
max_tokens = config.get("compression.max_tokens")

# Set value
config.set("compression.max_tokens", 2000, save=True)

# Update multiple
config.update({
    "compression.strategy": "mmr",
    "semantic_search.top_k_stm": 10
}, save=True)
```

### Use Langfuse Tracing
```python
from utils.langfuse_client import create_langfuse_client, LangfuseTracer

client = create_langfuse_client()
tracer = LangfuseTracer(client)

@tracer.trace_llm_call(model="gpt-4")
def generate_response(prompt):
    return llm.generate(prompt)

# Auto-traced!
response = generate_response("Hello")
client.flush()
```

## ðŸ—ï¸ Architecture

```mermaid
flowchart TD
    A[User Query] --> B[InputPreprocessor]
    B --> |embedding, intent, keywords| C{Memory Layers}
    
    C --> D[ShortTermMemory]
    C --> E[MidTermMemory]
    C --> F[LongTermMemory]
    
    D --> |recent messages| G[MemoryAggregator]
    E --> |summaries| G
    F --> |semantic search| G
    
    G --> |ranked contexts| H[ContextCompressor]
    H --> |compressed context| I[ResponseGenerator/LLM]
    I --> |raw response| J[ResponseSynthesizer]
    J --> |formatted response| K[Final Output]
```

## ðŸ“‚ Key Files

**Core:**
- `core/orchestrator.py` - Memory orchestration
- `core/compressor.py` - Context compression
- `core/aggregator.py` - Multi-layer aggregation

**Configuration:**
- `config/system_config.yaml` - Main config (200+ parameters)
- `config_ui.py` - Web UI for config adjustment
- `utils/config_manager.py` - Config management

**Tracing:**
- `utils/langfuse_client.py` - Langfuse integration
- `examples/langfuse_example.py` - Usage examples

**Testing:**
- `test_semantic_search.py` - Semantic search tests
- `test_comprehensive.py` - Full system tests
- `generate_embedded_data.py` - Test data generation

## ðŸ“š Documentation

**Essential Guides:**
- This README - Quick start & overview
- `config/system_config.yaml` - All configurable parameters (with comments)

**Setup Guides (if needed):**
- Neo4j: Run `python test_neo4j_connection.py` for help
- Langfuse: Edit `config/langfuse_config.yaml.example`

## ðŸ”¬ Experimentation

This is a **lab project** for testing different memory strategies:

```python
# Try different compression strategies
for strategy in ["score_based", "mmr", "truncate"]:
    config.set("compression.strategy", strategy, save=True)
    results = run_tests()
    print(f"{strategy}: {results}")

# Find optimal weights
for imp_weight in [0.3, 0.4, 0.5, 0.6]:
    config.set("compression.importance_weight", imp_weight, save=True)
    # Adjust other weights...
    results = run_tests()
```

## License

MIT License
